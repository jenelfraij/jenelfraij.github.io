---
layout: post
title: Blog Post 4
---

In this blog post, I will write a tutorial on a simple version of the *spectral clustering* algorithm for clustering data points. 

### Notation

Note that in all the math below: 

- Boldface capital letters like $\mathbf{A}$ refer to matrices. 
- Boldface lowercase letters like $\mathbf{v}$ refer to vectors. 
- $\mathbf{A}\mathbf{B}$ refers to a matrix-matrix product (`A@B`). $\mathbf{A}\mathbf{v}$ refers to a matrix-vector product (`A@v`). 


## Introduction

As mentioned above, in this blog post we'll study *spectral clustering*. Spectral clustering is an important tool for identifying meaningful parts of data sets with complex structure.

### Clustering example

Spectral clustering is a useful tool, however it is not always necessary. To start, we will look at an example where we don't need spectral clustering. 


```python
# import libraries
import numpy as np
from sklearn import datasets
from matplotlib import pyplot as plt
```


```python
n = 200 # we will have 200 data points
np.random.seed(1111) # set random seed to get same result
# generate isotropic Gaussian blobs for clustering. X contains the generated
# samples, and y contains the integer labels for cluster memberships.
X, y = datasets.make_blobs(n_samples=n, shuffle=True, random_state=None, centers = 2, cluster_std = 2.0)
plt.scatter(X[:,0], X[:,1]) # plot the generated values
```
![f1.png](/images/f1.png)

*Clustering* refers to the task of separating this data set into the two natural "blobs." K-means is a very common way to achieve this task, which has good performance on circular-ish blobs like these: 


```python
from sklearn.cluster import KMeans
km = KMeans(n_clusters = 2) # we want 2 clusters
km.fit(X) # fit the data into clusters
# plot the results with different colors for each cluster
plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```
![f2.png](/images/f2.png)

### Harder Clustering

It's not always that easy though. Sometimes our data could be "shaped weird". Let's take a look at a different example where we try to make clusters of moon shaped data.


```python
np.random.seed(1234)
n = 200 # number of data points
# make two interleaving half circles for clustering. X contains the generated
# samples, and y contains the integer labels for cluster memberships.
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.05, random_state=None)
plt.scatter(X[:,0], X[:,1]) # plot the data
```
![f3.png](/images/f3.png)

Visually, it looks like we can still make out two meaningful clusters in the data, but now we are dealing with crescents instead of bolbs. As before, the Euclidean coordinates of the data points are contained in the matrix `X`, while the labels for cluster memberships are contained in `y`. Now k-means won't work so well, because k-means is, by design, looking for circular clusters. Let's take a look.


```python
km = KMeans(n_clusters = 2) # we want 2 clusters
km.fit(X) # fit the data into clusters
plt.scatter(X[:,0], X[:,1], c = km.predict(X)) # plot clusters in different colors
```
![f4.png](/images/f4.png)

Uh oh that doesn't look so good :(

It seems like k-means is not always the answer, that is why we will look into an alternative way which is spectral clustering. In the following steps, we will derive and implement spectral clustering, and we will see how this method will be able to correctly cluster the two crescents.

## Part A

First of all, we will construct a similarity matrix with shape `(n, n)`, let's call it $\mathbf{A}$.

Now $\mathbf{A}$ is a matrix of zeros and ones; Entry `A[i,j]` will be equal to `1` if `X[i]` is within distance `epsilon` of `X[j]`, and `0` otherwise.In addition to that, we will set all the diagonal entries $\mathbf{A}$ to be zero regardless of distances. In other words, the matrix $\mathbf{A}$ contains information about which points are near (within distance `epsilon` of) other points.

To do that, we will use the function `pairwise_distances` from `sklearn.metrics` which computes all the pairwise from `X` distances and collect them into a distance matrix.

For this part, we will use `epsilon = 0.4`. 


```python
from sklearn import metrics
epsilon = 0.4
# we can create the matrix A by creating the distance matrix and checking if
# each distance value is less than espsilon. We multiply by 1 to get numerical
# values instead of True and False.
A = (metrics.pairwise_distances(X) < epsilon)*1
np.fill_diagonal(A, 0) # make sure that all diagonal values are zero.
A
```

```python
array([[0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 1, 0],
       ...,
       [0, 0, 0, ..., 0, 1, 1],
       [0, 0, 1, ..., 1, 0, 1],
       [0, 0, 0, ..., 1, 1, 0]])
```

## Part B

Now that $\mathbf{A}$ contains information about which points are near other points, we can use that to cluster the data points accordingly. First, letâ€™s define some mathematical expressions:

The *binary norm cut objective* of a matrix $\mathbf{A}$ is the function 

$$N_{\mathbf{A}}(C_0, C_1)\equiv \mathbf{cut}(C_0, C_1)\left(\frac{1}{\mathbf{vol}(C_0)} + \frac{1}{\mathbf{vol}(C_1)}\right)\;,$$

where $C_0$ and $C_1$ are the two clusters of the data points, $\mathbf{cut}(C_0, C_1)$ is the the *cut* of the clusters $C_0$ and $C_1$, and $\mathbf{vol}(C_0)$ is the *volume* of cluster $C_0$ as a measure of the size of the cluster.

Note that a pair of clusters $C_0$ and $C_1$ is considered to be a "good" partition of the data if $N_{\mathbf{A}}(C_0, C_1)$ is small. We will see why that is in the following steps.

Now to actually ind the value of $N_{\mathbf{A}}(C_0, C_1)$, we first need to define the $\mathbf{cut}$ and $\mathbf{vol}$ terms.


#### B.1 The Cut Term

The cut term $\mathbf{cut}(C_0, C_1)$ is defined as the number of nonzero entries in $\mathbf{A}$ that relate points in cluster $C_0$ to points in cluster $C_1$. We want this term to be small because that will essentially mean that points in $C_0$ aren't very close to points in $C_1$.

The cut term can be found using the equation $\mathbf{cut}(C_0, C_1) \equiv \sum_{i \in C_0, j \in C_1} a_{ij}$.

Let's write a function called `cut(A,y)` to compute the cut term using the above equation. We will do that by adding up the entries `A[i,j]` and `A[j,i]` for each pair of points `(i,j)` where `y[i]` and `y[j]` belong to different clusters.


```python
def cut(A,y):
    cut = 0 # initialize the summation to zero
    for i in range(n):
        # find the locations in y that are not in the same cluster as y[i]
        loc = np.where(y!=y[i])
        # add the entries of A in (i, these locations) to the sum.
        cut += np.sum(A[i, loc])
    return cut
```

Now let's see how we did. First, we compute the cut objective for the true clusters `y`. Then, we will generate an array of random labels of length `n` and check the cut objective for the random labels. We will compare the two to see if the cut objective for the true labels and see if it is much smaller than the cut objective for the random labels to check if this part of the cut objective gives better results for the true clusters over the random ones (which is what we want).


```python
rand_vec = np.random.randint(2, size = n) # generate random labels vector
cut(A, y), cut(A, rand_vec) # compare cuts using the two vectors
```

```python
(26, 2232)
```

We can see that the cut objective indeed gives much smaller results for the true clusters over the random ones. That's great news!

#### B.2 The Volume Term 

Now we will work on the second term in the norm cut objective, which is the *volume term*. We want neither of $C_0$ and $C_1$ to be too small because that will cause $\mathbf{vol}(C_0)$ or $\mathbf{vol}(C_1)$ to be small, and thus the fraction $\frac{1}{\mathbf{vol}(C_0)}$ or $\frac{1}{\mathbf{vol}(C_1)}$ will be large, leading to an undesirable higher objective value.


The volume term can be found using the equation $\mathbf{vol}(C_0) \equiv \sum_{i \in C_0}d_i$, where $d_i = \sum_{j = 1}^n a_{ij}$ is the degree of row $i$.

Let's write a function called `vols(A,y)` which computes the volumes of $C_0$ and $C_1$, using the equation above, and returns them as a tuple. 


```python
def vols(A,y):
    # return sum of rows of A that belong to each cluster
    return np.sum(A[y == 0]), np.sum(A[y == 1])
```

#### B.3 The Binary Normalized Cut Objective

Now that we have both the cut and volume terms, let's write a function called `normcut(A,y)` which uses `cut(A,y)` and `vols(A,y)` to compute the binary normalized cut objective of our matrix `A` with clustering vector `y`. 


```python
def normcut(A,y):
    # return the binary normalized cut objective accourding to the equation above
    return cut(A,y)*(1/vols(A,y)[0] + 1/vols(A,y)[1])
```

Now to make sure that we're doing okay, let's compare the `normcut` binary normalized cut objective using both the true labels `y` and the fake labels we generated above (`rand_vec`).


```python
normcut(A,y), normcut(A, rand_vec)
```

```python
(0.02303682466323045, 1.991672673470162)
```

Whoa this is really working! The binary normalized cut objective gives much better (smaller) results.

## Part C

Our goal now is to find a cluster vector `y` such that `normcut(A,y)` is small. This is an optimization problem. Let's define a new vector $\mathbf{z} \in \mathbb{R}^n$ such that: 

$$
z_i = 
\begin{cases}
    \frac{1}{\mathbf{vol}(C_0)} &\quad \text{if } y_i = 0 \\ 
    -\frac{1}{\mathbf{vol}(C_1)} &\quad \text{if } y_i = 1 \\ 
\end{cases}
$$


Now using linear algebra, we can represent the binary normalized cut objective in a different way.

$$\mathbf{N}_{\mathbf{A}}(C_0, C_1) = 2\frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}}\;,$$

where $\mathbf{D}$ is the diagonal matrix with nonzero entries $d_{ii} = d_i$, and  where $d_i = \sum_{j = 1}^n a_i$ is the degree from before. 

Note that we multiply the equation above by 2 because we defined your cut function by counting edges in both directions.

Now let's write a function called `transform(A,y)` that computes the $\mathbf{z}$ vector given our matrix `A` and vector of labels `y`, using the formula above.


```python
def transform(A,y):
    z = np.zeros(n) # create a zeros array of size n
    z[y==0] = 1/vols(A,y)[0] # set z = 1/vol(C_0) if y is 0
    z[y==1] = -1/vols(A,y)[1] # set z = 1/vol(C_1) if y is 1
    return z
```

Now let's check if this approach gives similar results of the binary normalized cut objective to the results we got from our first method. That is, we will compare the results of `normcut(A,y)` and $2\frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}}$.


```python
normcut1 = normcut(A,y) # get the normcut value using the first equation
z = transform(A,y) # get the z vector using our transform function
D = np.zeros((n, n)) # create an nxn zeros matrix D
np.fill_diagonal(D, sum(A)) # fill the diagonal of D with row-sum of A
normcut2 = 2*(z@(D-A)@z)/(z@D@z) # find the normcut value using the second equation
np.isclose(normcut1, normcut2) # check if the two values are equal
```

```python
True
```

And yep, they are equal!

Let's also check that $\mathbf{z}^T\mathbf{D}\mathbb{1} = 0$, where $\mathbb{1}$ is the vector of `n` ones. This implies that $\mathbf{z}$ contains roughly as many positive as negative entries. 


```python
z.T@D@(np.ones(n))
```

```python
-2.7755575615628914e-17
```

Which is approximately zero.

## Part D

Now it should be obvious that the problem of minimizing the normcut objective is equivalent to minimizing the function 

$$ R_\mathbf{A}(\mathbf{z})\equiv \frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}} $$

We will minimize $R_\mathbf{A}$ by minimizing the difference between $\mathbf{z}$ and its orthogonal component.

Here, we are using the code provided in the homework prompt to define an `orth_obj` function which handles this part. 


```python
def orth(u, v):
    return (u @ v) / (v @ v) * v

e = np.ones(n) 

d = D @ e

def orth_obj(z):
    z_o = z - orth(z, d)
    return (z_o @ (D - A) @ z_o)/(z_o @ D @ z_o)
```

Our next step is to minimize the function `orth_obj` with respect to $\mathbf{z}$ using the `minimize` function from `scipy.optimize`.


```python
from scipy.optimize import minimize
# minimize orth_obj with z as an initial guess. We can choose our miminizing
# method to be the Nelder-Mead algorithm for better results.
z_ = minimize(orth_obj, z, method="Nelder-Mead")
z_min = z_.x # we can get the minimized vector using .x
```

## Part E

Our minimized vector labels are not zeros and ones anymore, but the value of `z_min[i]` doesn't really matter because only the sign of `z_min[i]` contains information about the cluster label of data point `i`.

Let's plot the original data, using one color for points such that `z_min[i] < 0` and another color for points such that `z_min[i] >= 0`.


```python
plt.scatter(X[:,0], X[:,1], c = z_min < 0)
```
![f5.png](/images/f5.png)

We see that we obtain a great result! The two crescents are clustered perfectly.

## Part F

Since our calculations in part D aren't the most efficient, let's try to do it differently. In this part we will use eigenvalues and eigenvectors to perform the minimization.

It turns out that minimizing the function $R_\mathbf{A}$ as defined in part D is equivalent to finding the eigenvector with the second-smallest eigenvalue in the standard eigenvalue problem 

$$ \mathbf{D}^{-1}(\mathbf{D} - \mathbf{A}) \mathbf{z} = \lambda \mathbf{z}\;, \quad \mathbf{z}^T\mathbb{1} = 0\;.$$

Let's construct the (normalized) *Laplacian* matrix $\mathbf{L}$ of the similarity matrix $\mathbf{A}$.
$$\mathbf{L} = \mathbf{D}^{-1}(\mathbf{D} - \mathbf{A}).$$
We will then Find the eigenvector corresponding to its second-smallest eigenvalue, and call it `z_eig`.


```python
L = np.linalg.inv(D)@(D-A) # implement the equation of L
L = (L + L.T)/2 # symmetric version of L
Lam, U = np.linalg.eig(L) # get eigenvalues and corresponding eigenvectors
# find eigenvector associated with the second-smallest eigenvalue
z_eig = U.T[Lam == np.sort(Lam)[1]]
```

Then, we plot the data again, using the sign of `z_eig` as the color.


```python
plt.scatter(X[:,0], X[:,1], c = z_eig < 0)
```
![f6.png](/images/f6.png)

This is looking very good! The two crescents are clustered almost perfectly.

## Part G

Now, we can put together everything that weâ€™ve accomplished in the previous parts into one function called `spectral_clustering(X, epsilon)` which will perform spectral clustering for an input data `X` and the distance threshold `epsilon`. The function will return an array of `0` or `1` labels dependent on the cluster that each data point belongs to.


```python
def spectral_clustering(X, epsilon):
    """
    DESCRIPTION
    -----------
    This function performs spectral clustering, returning an array of binary
    labels indicating whether each data point is in cluster 0 or cluster 1.
    
    INPUT
    -----
    X: array of data points
    epsilon: distance threshold
    
    RETURN
    ------
    array of binary labels which indicate if a data point i is in group 0 or 1
    """
    
    A = (metrics.pairwise_distances(X) < epsilon)*1 # construct similarity matrix
    np.fill_diagonal(A, 0)
    D = np.zeros((n, n)) # construct diagonal matrix with degree entries
    np.fill_diagonal(D, sum(A))
    L = np.linalg.inv(D)@(D-A) # construct Laplacian matrix
    L = (L + L.T)/2 # symmetric version of L
    Lam, U = np.linalg.eig(L) # get eigenvalues and eigenvectors
    # eigenvector associated with the second-smallest eigenvalue
    z = U.T[Lam == np.sort(Lam)[1]]
    return (z > 0)*1
```

We can show that this function is working by plotting the data agian.


```python
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, epsilon))
```
![f7.png](/images/f7.png)

Looking good!

## Part H

We can generate different data sets to see if our function still works.

Letâ€™s make moons using:

1) n = 500, noise = 0.1

2) n = 1000, noise = 0.1

3) n = 1000, noise = 0.2


### n = 500, noise = 0.1


```python
n = 500
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.1, random_state=None)
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, 0.4))
```
![f8.png](/images/f8.png)

### n = 1000, noise = 0.1


```python
n = 1000
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.1, random_state=None)
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, 0.4))
```
![f9.png](/images/f9.png)

### n = 1000, noise = 0.2


```python
n = 1000
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.2, random_state=None)
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, 0.4))
```
![f10.png](/images/f10.png)

We can see that increasing the noise and the number of data points at the same time decreases the accuracy of spectral clustering. It still appears to work okay as the noise increases but the clusters are less distinct.

## Part I

Now let's try our spectral clustering function on another data set, the bull's eye!

Here's what it looks like.


```python
n = 1000
X, y = datasets.make_circles(n_samples=n, shuffle=True, noise=0.05, random_state=None, factor = 0.4)
plt.scatter(X[:,0], X[:,1])
```
![f11.png](/images/f11.png)

There are two concentric circles. As before k-means will not do well here at all. 


```python
km = KMeans(n_clusters = 2)
km.fit(X)
plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```
![f12.png](/images/f12.png)

Let's see if our function can successfully separate the two circles. We will do some experimentation with the value of `epsilon`.


```python
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, 0.7))
```
![f13.png](/images/f13.png)


```python
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, 0.5))
```
![f14.png](/images/f14.png)


```python
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, 0.3))
```
![f15.png](/images/f15.png)

It seems like a value of `epsilon` around 0.5 works best for the bullâ€™s eye plot.
